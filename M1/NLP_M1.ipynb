{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dataset Zip Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "with zipfile.ZipFile('./da7ee7_raw.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./da7ee7_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_by_split(text):\n",
    "    split_on = r'[,\\!\\.\\،\\:\\[\\]\\(\\)\\s\\'\\\"؟]'\n",
    "    tokenized_document=re.split(split_on,text)\n",
    "    tokenized_document=[token for token in tokenized_document if token!=\"\"]\n",
    "    return tokenized_document\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
    "    word_tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_all_documents():\n",
    "    all_documents= dict()\n",
    "    directory = os.path.join(os.getcwd(), \"da7ee7_raw\", \"Raw Data\")\n",
    "    for name in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, name)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                print(f\"Content of '{name}'\")\n",
    "                all_documents[name]=tokenize_by_split(f.read())\n",
    "                print(all_documents[name])\n",
    "        except (PermissionError, IsADirectoryError, UnicodeDecodeError) as e:\n",
    "            print(f\"Skipping '{name}' due to error: {e}\")\n",
    "        print()\n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with document name as key and array of tokens as value\n",
    "\n",
    "all_documents_tokenized= tokenize_all_documents() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "## use this method to ensure additional punctuations is removed & diacritics (tashkeel)\n",
    "def remove_punctuations_and_numbers(all_documents):\n",
    "    arabic_punctuation = '،؛؟«»ـ'\n",
    "    additional_punctuation = '[]\\\\'\n",
    "    all_punctuation = string.punctuation + arabic_punctuation + additional_punctuation\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u065F\\u0610-\\u061A\\u06D6-\\u06ED]')\n",
    "    \n",
    "    for document in all_documents:\n",
    "        filtered = []\n",
    "        for entry in all_documents[document]:\n",
    "            entry_no_diacritics = re.sub(arabic_diacritics, '', entry)\n",
    "            if (entry_no_diacritics and all(char not in all_punctuation for char in entry_no_diacritics) and not any(char.isdigit() for char in entry_no_diacritics)\n",
    "):\n",
    "                filtered.append(entry_no_diacritics)\n",
    "        all_documents[document] = filtered  \n",
    "                \n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_all_docs= remove_punctuations_and_numbers(all_documents_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_per_document(all_docs):\n",
    "    for document in all_docs:\n",
    "        tokens= all_docs[document]\n",
    "        word_count= dict()\n",
    "        for token in tokens:\n",
    "            if( not token in word_count):\n",
    "                word_count[token]=1\n",
    "            else:\n",
    "                word_count[token]+=1\n",
    "        all_docs[document]=word_count\n",
    "    return all_docs                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc= word_count_per_document(filtered_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing words appearing across all documents with highest counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_across_docs(unique_word_count):\n",
    "    print(\"before\")\n",
    "    all_words=dict()\n",
    "    print(\"after\")\n",
    "    for doc in unique_word_count:\n",
    "        tokens= unique_word_count[doc]\n",
    "        for token in tokens:\n",
    "            if (not token in all_words):\n",
    "                all_words[token]=tokens[token]\n",
    "            else: \n",
    "                all_words[token]+=tokens[token]    \n",
    "    return all_words            \n",
    "\n",
    "def choose_frequent_words_100(count_all_words):\n",
    "    stopwords=[]\n",
    "    for token in count_all_words:\n",
    "        if(count_all_words[token]>=100):\n",
    "            stopwords.append(token)\n",
    "    return stopwords    \n",
    "\n",
    "def choose_frequent_words_150(count_all_words):\n",
    "    stopwords=[]\n",
    "    for token in count_all_words:\n",
    "        if(count_all_words[token]>=150):\n",
    "            stopwords.append(token)\n",
    "    return stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Word Frequency Across All Documents (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not cleaned (every word kept same as in dataset)\n",
    "word_count_in_all_doc=count_word_across_docs(word_count_in_doc)\n",
    "stopwords_not_cleaned= choose_frequent_words_100(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_not_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_150=choose_frequent_words_150(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare words not >=150 but >=100 to see if relevant to remove\n",
    "\n",
    "def find_extra_words(most_frequent_100, most_frequent_150):\n",
    "    return set(most_frequent_100) - set(most_frequent_150)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_possible_stopwords= find_extra_words(stopwords_not_cleaned, greater_than_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_possible_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking word frequency after cleaning a bit (combining words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to see count difference (same word in multiple forms)\n",
    "def simplified_form(token):\n",
    "    found = True\n",
    "    base_word = \"\"\n",
    "    if token.startswith(\"و\"):  \n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"ب\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ك\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ف\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ل\") and len(token) > 3:  \n",
    "        base_word = token[1:]      \n",
    "    else:\n",
    "        found = False               \n",
    "    return base_word, found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_base_word(token):\n",
    "    found=True\n",
    "    base_word=\"\"\n",
    "    if token.startswith('وال'):  \n",
    "        base_word = token[3:]   \n",
    "    elif token.startswith('و') and len(token) > 1:  # Check if the word starts with 'و' and is not just 'و'\n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"ال\") and token!=\"اللى\": \n",
    "        base_word = token[2:] \n",
    "    elif token.startswith(\"بال\") and len(token) > 3:  \n",
    "        base_word = token[3:]  \n",
    "    elif token.startswith('ب') and len(token) > 3:  # cases like بشر\n",
    "        base_word = token[1:]     \n",
    "    elif token.startswith(\"كال\") and len(token) > 3:  \n",
    "        base_word = token[3:] \n",
    "    elif token.startswith(\"ك\") and len(token) > 2:  # cases like كل\n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"لل\") and len(token) > 2:  \n",
    "        base_word = token[2:]      \n",
    "    elif token.startswith(\"ل\") and len(token) > 1:  \n",
    "        base_word = token[1:]     \n",
    "    else:\n",
    "        found=False        \n",
    "    return base_word,found     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check across all documents الكلمة + with و\\ك\\ب\\ال\\ل\n",
    "\n",
    "def remove_duplicates_with_prefixes(word_count_per_doc):\n",
    "    all_words = set()  \n",
    "    removed_words_with_prefixes = []\n",
    "    for document in word_count_per_doc:\n",
    "        all_words.update(word_count_per_doc[document].keys())    \n",
    "    for document in word_count_per_doc:\n",
    "        tokens = word_count_per_doc[document]\n",
    "        keys_to_process = list(tokens.keys())  \n",
    "        for token in keys_to_process:\n",
    "            base_word,found= token_base_word(token)\n",
    "            second_base, second_match=simplified_form(token) #finding with ال if no presence of one without\n",
    "            if base_word in all_words and found:\n",
    "                if base_word not in tokens:\n",
    "                    tokens[base_word]=0\n",
    "                tokens[base_word] += tokens[token]\n",
    "                removed_words_with_prefixes.append(token)\n",
    "                del tokens[token]  \n",
    "            elif second_base in all_words and second_match:\n",
    "                if second_base not in tokens:\n",
    "                    tokens[second_base]=0\n",
    "                removed_words_with_prefixes.append(token)\n",
    "                tokens[second_base]+=tokens[token]  \n",
    "                del tokens[token]  \n",
    "    return word_count_per_doc,removed_words_with_prefixes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied on every words in each document separately\n",
    "# count_words_in_doc was a dictionary with key document name, value a dictionary of word in document \n",
    "# word in document is (key) and it's count as a value\n",
    "\n",
    "unique_word_count,removed_words_with_prefixes= remove_duplicates_with_prefixes(word_count_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we count similar words with prefixes removed across all documents again\n",
    "cleaned_count_per_doc= count_word_across_docs(unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are stopwards after prefixes were removed and count across doc recalculated\n",
    "stopwords_after_cleaning= choose_frequent_words_100(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_after_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 extra words after merging prefixes \n",
    "len(stopwords_after_cleaning)- len(stopwords_not_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_150_cleaned= choose_frequent_words_150(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwards_cleaned=find_extra_words(stopwords_after_cleaning,greater_than_150_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference is 189 between the cleaned_stopwards (>=100 word count) and (>=150 word count)\n",
    "extra_stopwards_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In how many documents was the same word considered frequent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding min values in IDF (most freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(term_document_count):\n",
    "    idf_values = dict()  # Dictionary to store the number of documents the word was present in\n",
    "    documents_count = len(term_document_count)\n",
    "    for doc in term_document_count:\n",
    "        tokens = term_document_count[doc]\n",
    "        for token in tokens:\n",
    "            if token not in idf_values:\n",
    "                idf_values[token] = 1\n",
    "            else:\n",
    "                idf_values[token] += 1  \n",
    "    for key in idf_values:\n",
    "        idf_values[key] = math.log10(documents_count / idf_values[key])     \n",
    "    sorted_idf = dict(sorted(idf_values.items(), key=lambda item: item[1]))\n",
    "    return sorted_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_100(idf_for_word):\n",
    "    top_100_freq=[]\n",
    "    counter=0\n",
    "    for entry in idf_for_word :\n",
    "        top_100_freq.append(entry)\n",
    "        counter=counter+1\n",
    "        if(counter==100):\n",
    "            break\n",
    "    return top_100_freq\n",
    "\n",
    "def find_top_150(idf_for_word):\n",
    "    top_150_freq=[]\n",
    "    counter=0\n",
    "    for entry in idf_for_word:\n",
    "        top_150_freq.append(entry)\n",
    "        counter=counter+1\n",
    "        if(counter==150):\n",
    "            break\n",
    "    return top_150_freq\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=calculate_idf(word_count_in_doc)\n",
    "idf_cleaned=calculate_idf(unique_word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 and 150 words that are frequent across documents (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_frequent_idf=find_top_100(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_150_frequent_idf=find_top_150(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 and 150 words that are frequent across documents (cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_freq_idf_cleaned= find_top_100(idf_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_150_freq_idf_cleaned= find_top_150(idf_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
