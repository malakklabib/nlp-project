{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mariam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir):\n",
    "    raw_data = {}\n",
    "    if os.path.exists(dir):\n",
    "        for file_name in os.listdir(dir):\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(dir, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    raw_data[file_name] = f.read()\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamps(text):\n",
    "    pattern = r'\\d+\\.\\d+:\\s*'\n",
    "    text_lines = text.split('\\n')\n",
    "    cleaned_lines = [re.sub(pattern, '', line) for line in text_lines]\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations_and_numbers(text):\n",
    "    arabic_punctuation = '،؛؟«»ـ'\n",
    "    additional_punctuation = '[]\\\\'\n",
    "    all_punctuation = string.punctuation + arabic_punctuation + additional_punctuation\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u065F\\u0610-\\u061A\\u06D6-\\u06ED]')\n",
    "    \n",
    "    # Split text into lines to preserve line breaks\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Remove diacritics\n",
    "        line = re.sub(arabic_diacritics, '', line)\n",
    "        \n",
    "        # Split into words and remove punctuation/numbers from each word\n",
    "        cleaned_words = []\n",
    "        words = line.split()\n",
    "        \n",
    "        for word in words:\n",
    "            # Remove any character that is punctuation or number\n",
    "            cleaned_word = ''.join(char for char in word \n",
    "                                 if char not in all_punctuation \n",
    "                                 and not char.isdigit())\n",
    "            # Only add non-empty words\n",
    "            if cleaned_word:\n",
    "                cleaned_words.append(cleaned_word)\n",
    "                \n",
    "        cleaned_lines.append(' '.join(cleaned_words))\n",
    "    \n",
    "    # Join lines back together with newlines\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_letters(text):\n",
    "    return re.sub(r'\\b[ء-ي]\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_words(text):\n",
    "    return re.sub(r'(\\b\\w+\\b)(\\s+\\1\\b\\s*)+', r'\\1 ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize_text(text):\n",
    "    pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
    "    word_tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def preprocess_transcripts(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    raw_data = load_data(input_dir)\n",
    "\n",
    "    for filename, text in raw_data.items():\n",
    "        text = remove_timestamps(text)\n",
    "        text = remove_punctuations_and_numbers(text)\n",
    "        text = remove_single_letters(text)\n",
    "        text = remove_repeated_words(text)\n",
    "\n",
    "        tokens = tokenize_text(text)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        # tokens = stem_tokens(tokens)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(' '.join(tokens))\n",
    "\n",
    "raw_dir = 'Raw Data'\n",
    "preprocessed_dir = 'Preprocessed Data'\n",
    "\n",
    "preprocess_transcripts(raw_dir, preprocessed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_combined_corpus(preprocessed_files):\n",
    "    corpus = list(preprocessed_files.values())\n",
    "    tfidf_matrix_all, vectorizer_all = compute_tfidf(corpus)\n",
    "    \n",
    "    df_all = pd.DataFrame(\n",
    "        tfidf_matrix_all.toarray(),\n",
    "        columns=vectorizer_all.get_feature_names_out(),\n",
    "        index=list(preprocessed_files.keys())\n",
    "    )\n",
    "    \n",
    "    df_all.to_csv('all_documents_tfidf.csv')\n",
    "\n",
    "preprocessed_files = load_data(preprocessed_dir)\n",
    "process_combined_corpus(preprocessed_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at AUBMindLab/bert-base-arabert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Preprocessed Data/مافيا  الدحيح.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Read text from file\u001b[39;00m\n\u001b[0;32m      8\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Preprocessed Data/مافيا  الدحيح.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m     10\u001b[0m     arabic_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Process text in chunks to avoid truncation\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Preprocessed Data/مافيا  الدحيح.txt'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a better Arabic NER model\n",
    "model_name = \"AUBMindLab/bert-base-arabert\" # AUBMindLab/bert-base-arabert\"  # Or try \"CAMeL-Lab/bert-base-arabic-camelbert-msa\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=model_name, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Read text from file\n",
    "file_path = \"./Preprocessed Data/مافيا  الدحيح.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    arabic_text = file.read()\n",
    "\n",
    "# Process text in chunks to avoid truncation\n",
    "max_length = 510  # Safe limit for transformer models\n",
    "words = arabic_text.split()\n",
    "chunks = [\" \".join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "\n",
    "# Process each chunk\n",
    "all_ner_results = []\n",
    "for chunk in chunks:\n",
    "    ner_results = ner_pipeline(chunk)\n",
    "    all_ner_results.extend(ner_results)\n",
    "\n",
    "    # Sort NER results by the 'start' position\n",
    "    sorted_ner_results = sorted(all_ner_results, key=lambda x: x['start'])\n",
    "\n",
    "    # Print sorted results\n",
    "    for entity in sorted_ner_results:\n",
    "        print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "# # Print results\n",
    "# for entity in all_ner_results:\n",
    "#     print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
