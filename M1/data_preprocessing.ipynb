{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dir):\n",
    "    raw_data = {}\n",
    "    if os.path.exists(dir):\n",
    "        for file_name in os.listdir(dir):\n",
    "            if file_name.endswith('.txt'):\n",
    "                file_path = os.path.join(dir, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    raw_data[file_name] = f.read()\n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Cleaning & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_timestamps(text):\n",
    "    pattern = r'\\d+\\.\\d+:\\s*'\n",
    "    text_lines = text.split('\\n')\n",
    "    cleaned_lines = [re.sub(pattern, '', line) for line in text_lines]\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations_and_numbers(text):\n",
    "    arabic_punctuation = '،؛؟«»ـ'\n",
    "    additional_punctuation = '[]\\\\'\n",
    "    all_punctuation = string.punctuation + arabic_punctuation + additional_punctuation\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u065F\\u0610-\\u061A\\u06D6-\\u06ED]')\n",
    "    \n",
    "    # Split text into lines to preserve line breaks\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Remove diacritics\n",
    "        line = re.sub(arabic_diacritics, '', line)\n",
    "        \n",
    "        # Split into words and remove punctuation/numbers from each word\n",
    "        cleaned_words = []\n",
    "        words = line.split()\n",
    "        \n",
    "        for word in words:\n",
    "            # Remove any character that is punctuation or number\n",
    "            cleaned_word = ''.join(char for char in word \n",
    "                                 if char not in all_punctuation \n",
    "                                 and not char.isdigit())\n",
    "            # Only add non-empty words\n",
    "            if cleaned_word:\n",
    "                cleaned_words.append(cleaned_word)\n",
    "                \n",
    "        cleaned_lines.append(' '.join(cleaned_words))\n",
    "    \n",
    "    # Join lines back together with newlines\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_letters(text):\n",
    "    return re.sub(r'\\b[ء-ي]\\b', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_words(text):\n",
    "    return re.sub(r'(\\b\\w+\\b)(\\s+\\1\\b\\s*)+', r'\\1 ', text).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize_text(text):\n",
    "    pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
    "    word_tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = [line.strip() for line in f]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def preprocess_transcripts(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    raw_data = load_data(input_dir)\n",
    "\n",
    "    for filename, text in raw_data.items():\n",
    "        text = remove_timestamps(text)\n",
    "        text = remove_punctuations_and_numbers(text)\n",
    "        text = remove_single_letters(text)\n",
    "        text = remove_repeated_words(text)\n",
    "\n",
    "        tokens = tokenize_text(text)\n",
    "        tokens = remove_stopwords(tokens)\n",
    "        #tokens = stem_tokens(tokens)\n",
    "        \n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(' '.join(tokens))\n",
    "\n",
    "raw_dir = 'Raw Data'\n",
    "preprocessed_dir = 'Preprocessed Data'\n",
    "\n",
    "preprocess_transcripts(raw_dir, preprocessed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into TF-IDF representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_combined_corpus(preprocessed_files):\n",
    "    corpus = list(preprocessed_files.values())\n",
    "    tfidf_matrix_all, vectorizer_all = compute_tfidf(corpus)\n",
    "    \n",
    "    df_all = pd.DataFrame(\n",
    "        tfidf_matrix_all.toarray(),\n",
    "        columns=vectorizer_all.get_feature_names_out(),\n",
    "        index=list(preprocessed_files.keys())\n",
    "    )\n",
    "    \n",
    "    df_all.to_csv('all_documents_tfidf.csv')\n",
    "\n",
    "preprocessed_files = load_data(preprocessed_dir)\n",
    "process_combined_corpus(preprocessed_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Use a better Arabic NER model\n",
    "model_name = \"AUBMindLab/bert-base-arabert\" # AUBMindLab/bert-base-arabert\"  # Or try \"CAMeL-Lab/bert-base-arabic-camelbert-msa\"\n",
    "ner_pipeline = pipeline(\"ner\", model=model_name, tokenizer=model_name, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Read text from file\n",
    "file_path = \"./Preprocessed Data/مافيا  الدحيح.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    arabic_text = file.read()\n",
    "\n",
    "# Process text in chunks to avoid truncation\n",
    "max_length = 510  # Safe limit for transformer models\n",
    "words = arabic_text.split()\n",
    "chunks = [\" \".join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "\n",
    "# Process each chunk\n",
    "all_ner_results = []\n",
    "for chunk in chunks:\n",
    "    ner_results = ner_pipeline(chunk)\n",
    "    all_ner_results.extend(ner_results)\n",
    "\n",
    "    # Sort NER results by the 'start' position\n",
    "    sorted_ner_results = sorted(all_ner_results, key=lambda x: x['start'])\n",
    "\n",
    "    # Print sorted results\n",
    "    for entity in sorted_ner_results:\n",
    "        print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "# # Print results\n",
    "# for entity in all_ner_results:\n",
    "#     print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
