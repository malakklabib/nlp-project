{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dataset Zip Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "with zipfile.ZipFile('./da7ee7_raw.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_by_split(text):\n",
    "    split_on = r'[,\\!\\.\\،\\:\\[\\]\\(\\)\\s\\'\\\"؟]'\n",
    "    tokenized_document=re.split(split_on,text)\n",
    "    tokenized_document=[token for token in tokenized_document if token!=\"\"]\n",
    "    return tokenized_document\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
    "    word_tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store document lengths\n",
    "\n",
    "\n",
    "def tokenize_all_documents(folder):\n",
    "    doc_lengths = []\n",
    "    all_documents= dict()\n",
    "    directory = os.path.join(os.getcwd(), folder)\n",
    "    for name in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, name)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                # print(f\"Content of '{name}'\")\n",
    "                all_documents[name]=tokenize_by_split(f.read())\n",
    "                doc_lengths.append(len(all_documents[name]))\n",
    "                # print(all_documents[name])\n",
    "        except (PermissionError, IsADirectoryError, UnicodeDecodeError) as e:\n",
    "            print(f\"Skipping '{name}' due to error: {e}\")\n",
    "        print()\n",
    "    return all_documents,doc_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with document name as key and array of tokens as value\n",
    "\n",
    "all_documents_tokenized, doc_lengths= tokenize_all_documents(\"Raw Data\") \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distribution(doc_lengths):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(doc_lengths, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Number of Words\")\n",
    "    plt.ylabel(\"Number of Documents\")\n",
    "    plt.title(\"Distribution of Document Lengths\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "## use this method to ensure additional punctuations is removed & diacritics (tashkeel)\n",
    "def remove_punctuations_and_numbers(all_documents):\n",
    "    arabic_punctuation = '،؛؟«»ـ'\n",
    "    additional_punctuation = '[]\\\\'\n",
    "    all_punctuation = string.punctuation + arabic_punctuation + additional_punctuation\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u065F\\u0610-\\u061A\\u06D6-\\u06ED]')\n",
    "    \n",
    "    for document in all_documents:\n",
    "        filtered = []\n",
    "        for entry in all_documents[document]:\n",
    "            entry_no_diacritics = re.sub(arabic_diacritics, '', entry)\n",
    "            if (entry_no_diacritics and all(char not in all_punctuation for char in entry_no_diacritics) and not any(char.isdigit() for char in entry_no_diacritics)\n",
    "):\n",
    "                filtered.append(entry_no_diacritics)\n",
    "        all_documents[document] = filtered  \n",
    "                \n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_all_docs= remove_punctuations_and_numbers(all_documents_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "def plot_bigram(all_docs):\n",
    "    all_bigrams = [bigram for doc in all_docs.values() if len(doc) > 1 for bigram in ngrams(doc, 2)]\n",
    "\n",
    "# Count the most common bigrams\n",
    "    bigram_freq = Counter(all_bigrams).most_common(30)  # Show top 20 bigrams\n",
    "\n",
    "# Prepare data for visualization\n",
    "    bigrams, counts = zip(*bigram_freq)  # Unpacking bigram and count\n",
    "\n",
    "# Properly reshape and align Arabic text\n",
    "    bigram_labels = [get_display(arabic_reshaper.reshape(\" \".join(bigram))) for bigram in bigrams]\n",
    "\n",
    "# Set Arabic font (make sure it's installed on your system)\n",
    "    plt.rcParams['font.family'] = 'Arial'  # You can replace 'Arial' with any Arabic-supporting font\n",
    "\n",
    "# Plot the top bigrams\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(bigram_labels[::-1], counts[::-1])  # Reverse for correct order\n",
    "    plt.xlabel(\"Frequency\", fontsize=14)\n",
    "    plt.title(\"Top 30 Bigrams\", fontsize=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bigram(filtered_all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_per_document(all_docs):\n",
    "    for document in all_docs:\n",
    "        tokens= all_docs[document]\n",
    "        word_count= dict()\n",
    "        for token in tokens:\n",
    "            if( not token in word_count):\n",
    "                word_count[token]=1\n",
    "            else:\n",
    "                word_count[token]+=1\n",
    "        all_docs[document]=word_count\n",
    "    return all_docs                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc= word_count_per_document(filtered_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing words appearing across all documents with highest counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_across_docs(unique_word_count):\n",
    "    all_words=dict()\n",
    "    for doc in unique_word_count:\n",
    "        tokens= unique_word_count[doc]\n",
    "        for token in tokens:\n",
    "            if (not token in all_words):\n",
    "                all_words[token]=tokens[token]\n",
    "            else: \n",
    "                all_words[token]+=tokens[token]    \n",
    "    return all_words            \n",
    "\n",
    "def choose_frequent_words_100(count_all_words):\n",
    "    stopwords=[]\n",
    "    for token in count_all_words:\n",
    "        if(count_all_words[token]>=100):\n",
    "            stopwords.append(token)\n",
    "    return stopwords    \n",
    "\n",
    "def choose_frequent_words_150(count_all_words):\n",
    "    stopwords=[]\n",
    "    for token in count_all_words:\n",
    "        if(count_all_words[token]>=150):\n",
    "            stopwords.append(token)\n",
    "    return stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Word Frequency Across All Documents (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not cleaned (every word kept same as in dataset)\n",
    "word_count_in_all_doc=count_word_across_docs(word_count_in_doc)\n",
    "stopwords_not_cleaned= choose_frequent_words_100(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_not_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to use later in preprocessing\n",
    "with open('stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in stopwords_not_cleaned:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_150=choose_frequent_words_150(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare words not >=150 but >=100 to see if relevant to remove\n",
    "\n",
    "def find_extra_words(most_frequent_100, most_frequent_150):\n",
    "    return set(most_frequent_100) - set(most_frequent_150)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_possible_stopwords= find_extra_words(stopwords_not_cleaned, greater_than_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_possible_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking word frequency after cleaning a bit (combining words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to see count difference (same word in multiple forms)\n",
    "def simplified_form(token):\n",
    "    found = True\n",
    "    base_word = \"\"\n",
    "    if token.startswith(\"و\"):  \n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"ب\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ك\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ف\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ل\") and len(token) > 3:  \n",
    "        base_word = token[1:]      \n",
    "    else:\n",
    "        found = False               \n",
    "    return base_word, found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_base_word(token):\n",
    "    found=True\n",
    "    base_word=\"\"\n",
    "    if token.startswith('وال'):  \n",
    "        base_word = token[3:]   \n",
    "    elif token.startswith('و') and len(token) > 1:  # Check if the word starts with 'و' and is not just 'و'\n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"ال\") and token!=\"اللى\": \n",
    "        base_word = token[2:] \n",
    "    elif token.startswith(\"بال\") and len(token) > 3:  \n",
    "        base_word = token[3:]  \n",
    "    elif token.startswith('ب') and len(token) > 3:  # cases like بشر\n",
    "        base_word = token[1:]     \n",
    "    elif token.startswith(\"كال\") and len(token) > 3:  \n",
    "        base_word = token[3:] \n",
    "    elif token.startswith(\"ك\") and len(token) > 2:  # cases like كل\n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"لل\") and len(token) > 2:  \n",
    "        base_word = token[2:]      \n",
    "    elif token.startswith(\"ل\") and len(token) > 1:  \n",
    "        base_word = token[1:]     \n",
    "    else:\n",
    "        found=False        \n",
    "    return base_word,found     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check across all documents الكلمة + with و\\ك\\ب\\ال\\ل\n",
    "\n",
    "def remove_duplicates_with_prefixes(word_count_per_doc):\n",
    "    all_words = set()  \n",
    "    removed_words_with_prefixes = []\n",
    "    for document in word_count_per_doc:\n",
    "        all_words.update(word_count_per_doc[document].keys())    \n",
    "    for document in word_count_per_doc:\n",
    "        tokens = word_count_per_doc[document]\n",
    "        keys_to_process = list(tokens.keys())  \n",
    "        for token in keys_to_process:\n",
    "            base_word,found= token_base_word(token)\n",
    "            second_base, second_match=simplified_form(token) #finding with ال if no presence of one without\n",
    "            if base_word in all_words and found:\n",
    "                if base_word not in tokens:\n",
    "                    tokens[base_word]=0\n",
    "                tokens[base_word] += tokens[token]\n",
    "                removed_words_with_prefixes.append(token)\n",
    "                del tokens[token]  \n",
    "            elif second_base in all_words and second_match:\n",
    "                if second_base not in tokens:\n",
    "                    tokens[second_base]=0\n",
    "                removed_words_with_prefixes.append(token)\n",
    "                tokens[second_base]+=tokens[token]  \n",
    "                del tokens[token]  \n",
    "    return word_count_per_doc,removed_words_with_prefixes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied on every words in each document separately\n",
    "# count_words_in_doc was a dictionary with key document name, value a dictionary of word in document \n",
    "# word in document is (key) and it's count as a value\n",
    "\n",
    "unique_word_count,removed_words_with_prefixes= remove_duplicates_with_prefixes(word_count_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we count similar words with prefixes removed across all documents again\n",
    "cleaned_count_per_doc= count_word_across_docs(unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are stopwards after prefixes were removed and count across doc recalculated\n",
    "stopwords_after_cleaning= choose_frequent_words_100(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_after_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 116 extra words after merging prefixes \n",
    "len(stopwords_after_cleaning)- len(stopwords_not_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for preprocessing\n",
    "all_stopwords = set(stopwords_not_cleaned + stopwords_after_cleaning)\n",
    "\n",
    "with open('final_stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in all_stopwords:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_150_cleaned= choose_frequent_words_150(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwards_cleaned=find_extra_words(stopwords_after_cleaning,greater_than_150_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference is 189 between the cleaned_stopwards (>=100 word count) and (>=150 word count)\n",
    "extra_stopwards_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In how many documents was the same word considered frequent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding min values in IDF (most freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(term_document_count):\n",
    "    idf_values = dict()  # Dictionary to store the number of documents the word was present in\n",
    "    documents_count = len(term_document_count)\n",
    "    for doc in term_document_count:\n",
    "        tokens = term_document_count[doc]\n",
    "        for token in tokens:\n",
    "            if token not in idf_values:\n",
    "                idf_values[token] = 1\n",
    "            else:\n",
    "                idf_values[token] += 1  \n",
    "    for key in idf_values:\n",
    "        idf_values[key] = math.log10(documents_count / idf_values[key])     \n",
    "    sorted_idf = dict(sorted(idf_values.items(), key=lambda item: item[1]))\n",
    "    return sorted_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_100(idf_for_word):\n",
    "    top_100_freq=[]\n",
    "    counter=0\n",
    "    for entry in idf_for_word :\n",
    "        top_100_freq.append(entry)\n",
    "        counter=counter+1\n",
    "        if(counter==100):\n",
    "            break\n",
    "    return top_100_freq\n",
    "\n",
    "def find_top_150(idf_for_word):\n",
    "    top_150_freq=[]\n",
    "    counter=0\n",
    "    for entry in idf_for_word:\n",
    "        top_150_freq.append(entry)\n",
    "        counter=counter+1\n",
    "        if(counter==150):\n",
    "            break\n",
    "    return top_150_freq\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=calculate_idf(word_count_in_doc)\n",
    "idf_cleaned=calculate_idf(unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(idf_values, documents):\n",
    "    scores = dict()\n",
    "    for doc in documents:\n",
    "        scores[doc] = dict()\n",
    "        for token in documents[doc]:  \n",
    "            tf = documents[doc][token] \n",
    "            idf = idf_values[token]\n",
    "            scores[doc][token] = tf * idf  \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_values=compute_tf_idf(idf,word_count_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_top_words(tf_idf_scores, top_n=10):\n",
    "    top_scores = {}\n",
    "    for doc, scores in tf_idf_scores.items():\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_scores[doc] = dict(sorted_scores[:top_n])\n",
    "    return top_scores\n",
    "\n",
    "def reshape_arabic_text(text):\n",
    "    return get_display(arabic_reshaper.reshape(text))\n",
    "\n",
    "def reshape_arabic_words(words):\n",
    "    reshaped_words = [get_display(arabic_reshaper.reshape(word)) for word in words]\n",
    "    return reshaped_words\n",
    "\n",
    "def create_individual_heatmaps(top_scores):\n",
    "    for doc, scores in top_scores.items():\n",
    "        reshaped_doc = reshape_arabic_text(doc)  \n",
    "        words = list(scores.keys())\n",
    "        reshaped_words = reshape_arabic_words(words)  \n",
    "        data = pd.DataFrame.from_dict(scores, orient=\"index\", columns=[reshaped_doc])\n",
    "        data.index = reshaped_words \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(data, cmap=\"YlGnBu\", annot=True, fmt=\".2f\", cbar_kws={\"label\": \"TF-IDF Score\"})\n",
    "        plt.title(f\"TF-IDF Heatmap for {reshaped_doc}\")\n",
    "        plt.xlabel(\"Document\")\n",
    "        plt.ylabel(\"Words\")\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores=get_top_words(tf_idf_values)\n",
    "create_individual_heatmaps(top_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 and 150 words that are frequent across documents (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_frequent_idf=find_top_100(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_150_frequent_idf=find_top_150(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 and 150 words that are frequent across documents (cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_freq_idf_cleaned= find_top_100(idf_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_150_freq_idf_cleaned= find_top_150(idf_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud For (all docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "def generate_wordcloud_from_dict(word_counts):\n",
    "    font_directory = os.path.join(os.getcwd(), \"font\")\n",
    "    for file in os.listdir(font_directory):\n",
    "        if file.endswith(\".ttf\"): \n",
    "            font_file = os.path.join(font_directory, file)\n",
    "            break\n",
    "    if not font_file:\n",
    "        raise FileNotFoundError(\"No .ttf font file found in the specified directory!\")\n",
    "    \n",
    "    reshaped_word_counts = {\n",
    "        get_display(arabic_reshaper.reshape(word)): count for word, count in word_counts.items()\n",
    "    }\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        font_path=font_file,\n",
    "        width=5000,\n",
    "        height=4000,\n",
    "        random_state=1,\n",
    "        background_color='white',\n",
    "        colormap='Set2'\n",
    "    ).generate_from_frequencies(reshaped_word_counts)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_from_dict(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_from_dict(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud for Rare Words in All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud_rare(idf, word_count_in_all_doc):\n",
    "    reversed_idf = {key: idf[key] for key in reversed(idf)}\n",
    "    rare=dict()\n",
    "    counter=0\n",
    "    for entry in reversed_idf:\n",
    "        rare[entry]=word_count_in_all_doc[entry]\n",
    "        counter+=1\n",
    "        if(counter==100):\n",
    "            break\n",
    "    generate_wordcloud_from_dict(rare) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_rare(idf,word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_rare(idf_cleaned,cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_all_documents_after, doc_lengths_after= tokenize_all_documents(\"Preprocessed Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(doc_lengths_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bigram(tokenize_all_documents_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc_after= word_count_per_document(tokenize_all_documents_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_words_after= count_word_across_docs(word_count_in_doc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_words_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_frequent_words_60(all_docs):\n",
    "    take_Word=[]\n",
    "    for word in all_docs:\n",
    "        if(all_docs[word]>=60):\n",
    "            take_Word.append(word)\n",
    "    return take_Word        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_frequent_words_60(all_doc_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_after=calculate_idf(word_count_in_doc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_from_dict(all_doc_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_rare(idf_after,all_doc_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_values_after= compute_tf_idf(idf_after,word_count_in_doc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores_after= get_top_words(tf_idf_values_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_individual_heatmaps(top_scores_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
