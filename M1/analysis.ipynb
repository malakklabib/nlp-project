{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dataset Zip Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "with zipfile.ZipFile('./da7ee7_raw.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile('./annotations.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Categories to exclude (e.g., \"الدحيح\" and its variants)\n",
    "categories_to_merge = {\n",
    "    \"الدحيح\",\n",
    "    \"eldahih\",\n",
    "    \"elda7ee7\",\n",
    "    \"da7ee7\",\n",
    "    \"al daheeh\",\n",
    "    \"eldaheeh\",\n",
    "    \"new media academy\",\n",
    "    \"دحيح\",\n",
    "    \"daheeh\",\n",
    "    \"#daheeh #الدحيح\",\n",
    "    \"برنامج الدحيح\",\n",
    "    \"new media academy الدحيح\",\n",
    "    \"برنامج الدحيح الجديد\",\n",
    "    \"elda7i7\",\n",
    "    \"حلقة الدحيح\",\n",
    "    \"اكاديمية الاعلام الجديد\"\n",
    "\n",
    "}\n",
    "\n",
    "def clean_categories(categories):\n",
    "    cleaned = set()\n",
    "    for cat in categories:\n",
    "        if cat in categories_to_merge:\n",
    "            continue\n",
    "        cleaned.add(cat.strip())\n",
    "    return cleaned\n",
    "\n",
    "def process_annotations(folder):\n",
    "    result = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\"_annotation.json\"):\n",
    "            filepath = os.path.join(folder, filename)\n",
    "\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                # Normalize and clean categories\n",
    "                categories = set(data.get(\"Categories\", []))\n",
    "                normalized_categories = clean_categories(categories)\n",
    "\n",
    "                # Build the dictionary entry\n",
    "                base_filename = filename.replace(\"_annotation.json\", \".txt\")\n",
    "                result[base_filename] = {\n",
    "                    \"categories\": list(normalized_categories),\n",
    "                    \"likes\": data.get(\"Number_of_Likes\", 0),\n",
    "                    \"views\": data.get(\"Number_of_Views\", 0),\n",
    "                    \"dislikes\": data.get(\"Number_of_Dislikes\", 0),\n",
    "                }\n",
    "    return result\n",
    "\n",
    "annotations_dict = process_annotations(\"Annotations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_most_liked_disliked(data):\n",
    "    documents_summary = []\n",
    "\n",
    "    for doc_name, details in data.items():\n",
    "        documents_summary.append({\n",
    "            \"Document Name\": doc_name,\n",
    "             \"Views\":details.get(\"views\", 0),\n",
    "            \"Categories\": details.get(\"categories\", []),\n",
    "            \"Likes\": details.get(\"likes\", 0),\n",
    "            \"Dislikes\": details.get(\"dislikes\", 0)\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(documents_summary)\n",
    "    most_liked = df.loc[df[\"Likes\"].idxmax()]\n",
    "    most_disliked = df.loc[df[\"Dislikes\"].idxmax()]\n",
    "\n",
    "    return most_liked, most_disliked\n",
    "\n",
    "def display_results(most_liked, most_disliked):\n",
    "    print(\"Most Liked Document:\")\n",
    "    print(f\"  Document Name: {most_liked['Document Name']}\")\n",
    "    print(f\"  Views: {most_liked['Views']}\")\n",
    "    print(f\"  Likes: {most_liked['Likes']}\")\n",
    "    print(f\"  Categories: {', '.join(most_liked['Categories'])}\\n\")\n",
    "\n",
    "    print(\"Most Liked Document:\")\n",
    "    print(f\"  Document Name: {most_disliked['Document Name']}\")\n",
    "    print(f\"  Views: {most_disliked['Views']}\")\n",
    "    print(f\"  Dislikes: {most_disliked['Dislikes']}\")\n",
    "    print(f\"  Categories: {', '.join(most_disliked['Categories'])}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "most_liked_doc, most_disliked_doc = get_most_liked_disliked(annotations_dict)\n",
    "display_results(most_liked_doc, most_disliked_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display  # For proper display of reshaped Arabic text\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert annotations_dict to a DataFrame\n",
    "def annotations_to_dataframe(annotations_dict):\n",
    "    data = []\n",
    "    for doc, details in annotations_dict.items():\n",
    "        categories = details.get(\"categories\", [])\n",
    "        data.append({\n",
    "            \"Document\": doc,\n",
    "            \"Categories\": categories\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Count unique occurrences of each category across all documents\n",
    "def calculate_category_counts(df):\n",
    "    global_category_counts = {}\n",
    "    seen_categories = set()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        document_categories = row[\"Categories\"]\n",
    "        for category in document_categories:\n",
    "            # Skip this category if it's already counted for this document\n",
    "            if category in seen_categories:\n",
    "                continue\n",
    "\n",
    "            # Check if this category contains any word from already taken categories\n",
    "            category_words = set(category.split())\n",
    "            skip_category = False\n",
    "            for seen_category in seen_categories:\n",
    "                seen_words = set(seen_category.split())\n",
    "                if category_words & seen_words:  \n",
    "                    skip_category = True\n",
    "                    break\n",
    "            \n",
    "            if skip_category:\n",
    "                continue\n",
    "\n",
    "            # Count the category globally if it appears in any document other than the current one\n",
    "            global_count = sum(1 for _, doc_row in df.iterrows() if category in doc_row[\"Categories\"])\n",
    "            if global_count > 1:\n",
    "                global_category_counts[category] = global_count\n",
    "            seen_categories.add(category)\n",
    "\n",
    "    return global_category_counts\n",
    "\n",
    "def display_top_categories(category_counts, top_n=20):\n",
    "    sorted_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    print(f\"{'Count':>10} {'Category':<50}\") \n",
    "    print(\"=\" * 60)\n",
    "    for category, count in sorted_categories:\n",
    "        print(f\"{count:>10} {category:<50}\")  \n",
    "    \n",
    "    return sorted_categories\n",
    "\n",
    "def plot_top_categories(top_categories):\n",
    "    categories, counts = zip(*top_categories)\n",
    "\n",
    "    reshaped_categories = [\n",
    "        get_display(arabic_reshaper.reshape(category)) if any(\"\\u0600\" <= char <= \"\\u06FF\" for char in category) else category\n",
    "        for category in categories\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(reshaped_categories, counts, color='skyblue')\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=14)\n",
    "    plt.title(\"Top 20 Categories by Count\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df_annotations = annotations_to_dataframe(annotations_dict)\n",
    "\n",
    "if not df_annotations.empty:\n",
    "    category_counts = calculate_category_counts(df_annotations)\n",
    "    top_categories = display_top_categories(category_counts, top_n=20)\n",
    "    plot_top_categories(top_categories)\n",
    "else:\n",
    "    print(\"No data to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_by_split(text):\n",
    "    split_on = r'[,\\!\\.\\،\\:\\[\\]\\(\\)\\s\\'\\\"؟]'\n",
    "    tokenized_document=re.split(split_on,text)\n",
    "    tokenized_document=[token for token in tokenized_document if token!=\"\"]\n",
    "    return tokenized_document\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    pattern = r'\\b\\w+\\b|[^\\w\\s]'\n",
    "    word_tokenizer = RegexpTokenizer(pattern)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store document lengths\n",
    "\n",
    "\n",
    "def tokenize_all_documents(folder):\n",
    "    doc_lengths = []\n",
    "    all_documents= dict()\n",
    "    directory = os.path.join(os.getcwd(), folder)\n",
    "    for name in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, name)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                # print(f\"Content of '{name}'\")\n",
    "                all_documents[name]=tokenize_by_split(f.read())\n",
    "                doc_lengths.append(len(all_documents[name]))\n",
    "                # print(all_documents[name])\n",
    "        except (PermissionError, IsADirectoryError, UnicodeDecodeError) as e:\n",
    "            print(f\"Skipping '{name}' due to error: {e}\")\n",
    "        print()\n",
    "    return all_documents,doc_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store document lengths\n",
    "def get_all_documents(folder):\n",
    "    all_documents= dict()\n",
    "    directory = os.path.join(os.getcwd(), folder)\n",
    "    for name in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, name)\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                # print(f\"Content of '{name}'\")\n",
    "                all_documents[name]=f.read()\n",
    "                # print(all_documents[name])\n",
    "        except (PermissionError, IsADirectoryError, UnicodeDecodeError) as e:\n",
    "            print(f\"Skipping '{name}' due to error: {e}\")\n",
    "        print()\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary with document name as key and array of tokens as value\n",
    "\n",
    "all_documents_tokenized, doc_lengths= tokenize_all_documents(\"Raw Data\") \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_distribution(doc_lengths):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.hist(doc_lengths, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "    plt.xlabel(\"Number of Words\")\n",
    "    plt.ylabel(\"Number of Documents\")\n",
    "    plt.title(\"Distribution of Document Lengths\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(doc_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "## use this method to ensure additional punctuations is removed & diacritics (tashkeel)\n",
    "def remove_punctuations_and_numbers(all_documents):\n",
    "    arabic_punctuation = '،؛؟«»ـ'\n",
    "    additional_punctuation = '[]\\\\'\n",
    "    all_punctuation = string.punctuation + arabic_punctuation + additional_punctuation\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u065F\\u0610-\\u061A\\u06D6-\\u06ED]')\n",
    "    \n",
    "    for document in all_documents:\n",
    "        filtered = []\n",
    "        for entry in all_documents[document]:\n",
    "            entry_no_diacritics = re.sub(arabic_diacritics, '', entry)\n",
    "            if (entry_no_diacritics and all(char not in all_punctuation for char in entry_no_diacritics) and not any(char.isdigit() for char in entry_no_diacritics)\n",
    "):\n",
    "                filtered.append(entry_no_diacritics)\n",
    "        all_documents[document] = filtered  \n",
    "                \n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_all_docs= remove_punctuations_and_numbers(all_documents_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_all_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove stop words and unwanted characters\n",
    "def remove_stopwords(all_docs):\n",
    "    # Define Arabic stop words\n",
    "    stop_words = set([\n",
    "        \"يا\", \"دلوقتي\", \"بعد\", \"ما\", \"ايه\", \"كل\", \"ده\", \"وما\", \"في\", \"بس\", \"من\",\n",
    "        \"او\", \"اللي\", \"فيش\", \"ولا\", \"الله\", \"انت\", \"على\", \"تاني\", \"لو\", \"مش\", \"انا\", \"حد\", \"مع\",\n",
    "        \"هذه\", \"هو\", \"عليه\", \"ان\", \"مثلا\", \"دول\", \"كانوا\", \"ممكن\", \"عزيزي\", \"دي\", \"ازاي\", \"غير\",\n",
    "        \"طبعا\", \"بعض\", \"كمان\", \"ولكن\", \"كده\", \"كتير\", \"زي\", \"قبل\", \"حصل\", \"لكن\", \"كانش\", \"فيه\",\n",
    "        \"فيها\", \"لازم\",  \"عن\", \"وفي\", \"له\", \"طب\", \"احنا\",  \"عليها\", \"منه\",\n",
    "        \"بين\", \"وهو\", \"عنده\",  \"بقى\", \"لما\", \"حتى\", \"هذا\", \"حسب\",  \n",
    "        \"كان\",  \"واللي\",  \"بتاع\", \"عايز\", \"ليه\",\n",
    "        \"بيقول\", \"برضه\", \"دا\", \"إن\", \"العالم\", \"أوي\", \"أنا\",  \"أي\",\n",
    "    \"إيه\",   \"حاجة\", \"تانية\", \"كدا\", \"سنة\", \"و\", \"أو\", \"إنه\", \"وقتها\", \"أكتر\",\n",
    "         \"لأن\",\"هما\", \"ودا\", \"لسة\", \"شوية\"\n",
    "    ])\n",
    "    \n",
    "    # Arabic punctuation to remove\n",
    "    arabic_punctuation = \"؟،؛ـ:\"\n",
    "\n",
    "    # Process each document\n",
    "    cleaned_docs = {}\n",
    "    for doc_name, text in all_docs.items():\n",
    "        # Remove numbers, commas, and Arabic punctuation\n",
    "        text = re.sub(r\"[0-9,{}]\".format(arabic_punctuation), \"\", text)\n",
    "        \n",
    "        # Split text into words and remove stop words\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Reconstruct the document text\n",
    "        cleaned_docs[doc_name] = \" \".join(filtered_words)\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "\n",
    "copy_dict=remove_stopwords(get_all_documents(\"Raw Data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('Ammar-alhaj-ali/arabic-MARBERT-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('Ammar-alhaj-ali/arabic-MARBERT-sentiment')\n",
    "print(\"Tokenizer and model loaded successfully.\\n\")\n",
    "\n",
    "def analyze_sentiments(documents, words_per_chunk=150):\n",
    "    results = {}\n",
    "\n",
    "    for doc_name, words in documents.items():\n",
    "        print(f\"Processing document: {doc_name}...\")\n",
    "        chunks = [' '.join(words[i:i + words_per_chunk]) for i in range(0, len(words), words_per_chunk)]\n",
    "        chunk_results = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "           \n",
    "            inputs = tokenizer(chunk, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probabilities = logits.softmax(dim=-1).squeeze().tolist()  \n",
    "            predicted_class = logits.argmax(dim=-1).item()\n",
    "\n",
    "            # Append results\n",
    "            chunk_results.append((probabilities, predicted_class))\n",
    "\n",
    "        # Step 3: Aggregate results from all chunks\n",
    "        sentiments = [result[1] for result in chunk_results]\n",
    "        avg_probabilities = [sum(chunk[0][i] for chunk in chunk_results) / len(chunk_results) for i in range(3)]\n",
    "        final_sentiment = max(set(sentiments), key=sentiments.count)\n",
    "        labels = ['negative', 'neutral', 'positive']  \n",
    "        print(f\"Aggregated Sentiment: {labels[final_sentiment]} \\n\")\n",
    "\n",
    "        # Store results\n",
    "        results[doc_name] = {\n",
    "            'text': ' '.join(words[:150]) + '...',\n",
    "            'sentiment': labels[final_sentiment],\n",
    "            'probabilities': avg_probabilities\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "results = analyze_sentiments(copy_dict)\n",
    "\n",
    "print(\"\\nFinal Sentiment Analysis Results:\")\n",
    "for doc_name, analysis in results.items():\n",
    "    print(f\"Document: {doc_name}\")\n",
    "    print(f\"Text: {analysis['text']}\")\n",
    "    print(f\"Sentiment: {analysis['sentiment']}\")\n",
    "    print(f\"Probabilities: {analysis['probabilities']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "def plot_bigram(all_docs):\n",
    "    all_bigrams = [bigram for doc in all_docs.values() if len(doc) > 1 for bigram in ngrams(doc, 2)]\n",
    "\n",
    "# Count the most common bigrams\n",
    "    bigram_freq = Counter(all_bigrams).most_common(30)  # Show top 20 bigrams\n",
    "\n",
    "# Prepare data for visualization\n",
    "    bigrams, counts = zip(*bigram_freq)  # Unpacking bigram and count\n",
    "\n",
    "# Properly reshape and align Arabic text\n",
    "    bigram_labels = [get_display(arabic_reshaper.reshape(\" \".join(bigram))) for bigram in bigrams]\n",
    "\n",
    "# Set Arabic font (make sure it's installed on your system)\n",
    "    plt.rcParams['font.family'] = 'Arial'  # You can replace 'Arial' with any Arabic-supporting font\n",
    "\n",
    "# Plot the top bigrams\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(bigram_labels[::-1], counts[::-1])  # Reverse for correct order\n",
    "    plt.xlabel(\"Frequency\", fontsize=14)\n",
    "    plt.title(\"Top 30 Bigrams\", fontsize=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bigram(filtered_all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating TDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_per_document(all_docs):\n",
    "    for document in all_docs:\n",
    "        tokens= all_docs[document]\n",
    "        word_count= dict()\n",
    "        for token in tokens:\n",
    "            if( not token in word_count):\n",
    "                word_count[token]=1\n",
    "            else:\n",
    "                word_count[token]+=1\n",
    "        all_docs[document]=word_count\n",
    "    return all_docs                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc= word_count_per_document(filtered_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing words appearing across all documents with highest counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_across_docs(unique_word_count):\n",
    "    all_words=dict()\n",
    "    for doc in unique_word_count:\n",
    "        tokens= unique_word_count[doc]\n",
    "        for token in tokens:\n",
    "            if (not token in all_words):\n",
    "                all_words[token]=tokens[token]\n",
    "            else: \n",
    "                all_words[token]+=tokens[token]    \n",
    "    return all_words            \n",
    "\n",
    "def choose_frequent_words_350(count_all_words):\n",
    "    stopwords=[]\n",
    "    for token in count_all_words:\n",
    "        if(count_all_words[token]>=350):\n",
    "            stopwords.append(token)\n",
    "    return stopwords    \n",
    "\n",
    "def choose_frequent_words_550(count_all_words):\n",
    "    stopwords=[]\n",
    "    for token in count_all_words:\n",
    "        if(count_all_words[token]>=550):\n",
    "            stopwords.append(token)\n",
    "    return stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Word Frequency Across All Documents (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## not cleaned (every word kept same as in dataset)\n",
    "word_count_in_all_doc=count_word_across_docs(word_count_in_doc)\n",
    "stopwords_not_cleaned= choose_frequent_words_350(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_all_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_not_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to use later in preprocessing\n",
    "with open('stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in stopwords_not_cleaned:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_550=choose_frequent_words_550(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare words not >=150 but >=100 to see if relevant to remove\n",
    "\n",
    "def find_extra_words(most_frequent_350, most_frequent_550):\n",
    "    return set(most_frequent_350) - set(most_frequent_550)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_possible_stopwords= find_extra_words(stopwords_not_cleaned, greater_than_550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_possible_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking word frequency after cleaning a bit (combining words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trying to see count difference (same word in multiple forms)\n",
    "def simplified_form(token):\n",
    "    found = True\n",
    "    base_word = \"\"\n",
    "    if token.startswith(\"و\"):  \n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"ب\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ك\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ف\") and len(token) > 3:  \n",
    "        base_word = token[1:]  \n",
    "    elif token.startswith(\"ل\") and len(token) > 3:  \n",
    "        base_word = token[1:]      \n",
    "    else:\n",
    "        found = False               \n",
    "    return base_word, found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_base_word(token):\n",
    "    found=True\n",
    "    base_word=\"\"\n",
    "    if token.startswith('وال'):  \n",
    "        base_word = token[3:]   \n",
    "    elif token.startswith('و') and len(token) > 1:  # Check if the word starts with 'و' and is not just 'و'\n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"ال\") and token!=\"اللى\": \n",
    "        base_word = token[2:] \n",
    "    elif token.startswith(\"بال\") and len(token) > 3:  \n",
    "        base_word = token[3:]  \n",
    "    elif token.startswith('ب') and len(token) > 3:  # cases like بشر\n",
    "        base_word = token[1:]     \n",
    "    elif token.startswith(\"كال\") and len(token) > 3:  \n",
    "        base_word = token[3:] \n",
    "    elif token.startswith(\"ك\") and len(token) > 2:  # cases like كل\n",
    "        base_word = token[1:] \n",
    "    elif token.startswith(\"لل\") and len(token) > 2:  \n",
    "        base_word = token[2:]      \n",
    "    elif token.startswith(\"ل\") and len(token) > 1:  \n",
    "        base_word = token[1:]     \n",
    "    else:\n",
    "        found=False        \n",
    "    return base_word,found     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check across all documents الكلمة + with و\\ك\\ب\\ال\\ل\n",
    "\n",
    "def remove_duplicates_with_prefixes(word_count_per_doc):\n",
    "    all_words = set()  \n",
    "    removed_words_with_prefixes = []\n",
    "    for document in word_count_per_doc:\n",
    "        all_words.update(word_count_per_doc[document].keys())    \n",
    "    for document in word_count_per_doc:\n",
    "        tokens = word_count_per_doc[document]\n",
    "        keys_to_process = list(tokens.keys())  \n",
    "        for token in keys_to_process:\n",
    "            base_word,found= token_base_word(token)\n",
    "            second_base, second_match=simplified_form(token) #finding with ال if no presence of one without\n",
    "            if base_word in all_words and found:\n",
    "                if base_word not in tokens:\n",
    "                    tokens[base_word]=0\n",
    "                tokens[base_word] += tokens[token]\n",
    "                removed_words_with_prefixes.append(token)\n",
    "                del tokens[token]  \n",
    "            elif second_base in all_words and second_match:\n",
    "                if second_base not in tokens:\n",
    "                    tokens[second_base]=0\n",
    "                removed_words_with_prefixes.append(token)\n",
    "                tokens[second_base]+=tokens[token]  \n",
    "                del tokens[token]  \n",
    "    return word_count_per_doc,removed_words_with_prefixes\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied on every words in each document separately\n",
    "# count_words_in_doc was a dictionary with key document name, value a dictionary of word in document \n",
    "# word in document is (key) and it's count as a value\n",
    "\n",
    "unique_word_count,removed_words_with_prefixes= remove_duplicates_with_prefixes(word_count_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we count similar words with prefixes removed across all documents again\n",
    "cleaned_count_per_doc= count_word_across_docs(unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are stopwards after prefixes were removed and count across doc recalculated\n",
    "stopwords_after_cleaning= choose_frequent_words_350(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_after_cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17 extra words after merging prefixes \n",
    "len(stopwords_after_cleaning)- len(stopwords_not_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greater_than_550_cleaned= choose_frequent_words_550(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwards_cleaned=find_extra_words(stopwords_after_cleaning,greater_than_550_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_stopwards_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In how many documents was the same word considered frequent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding min values in IDF (most freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(term_document_count):\n",
    "    idf_values = dict()  # Dictionary to store the number of documents the word was present in\n",
    "    documents_count = len(term_document_count)\n",
    "    for doc in term_document_count:\n",
    "        tokens = term_document_count[doc]\n",
    "        for token in tokens:\n",
    "            if token not in idf_values:\n",
    "                idf_values[token] = 1\n",
    "            else:\n",
    "                idf_values[token] += 1  \n",
    "    for key in idf_values:\n",
    "        idf_values[key] = math.log10(documents_count / idf_values[key])     \n",
    "    sorted_idf = dict(sorted(idf_values.items(), key=lambda item: item[1]))\n",
    "    return sorted_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_100(idf_for_word):\n",
    "    top_100_freq=[]\n",
    "    counter=0\n",
    "    for entry in idf_for_word :\n",
    "        top_100_freq.append(entry)\n",
    "        counter=counter+1\n",
    "        if(counter==100):\n",
    "            break\n",
    "    return top_100_freq\n",
    "\n",
    "def find_top_150(idf_for_word):\n",
    "    top_150_freq=[]\n",
    "    counter=0\n",
    "    for entry in idf_for_word:\n",
    "        top_150_freq.append(entry)\n",
    "        counter=counter+1\n",
    "        if(counter==150):\n",
    "            break\n",
    "    return top_150_freq\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf=calculate_idf(word_count_in_doc)\n",
    "idf_cleaned=calculate_idf(unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(idf_values, documents):\n",
    "    scores = dict()\n",
    "    for doc in documents:\n",
    "        scores[doc] = dict()\n",
    "        for token in documents[doc]:  \n",
    "            tf = documents[doc][token] \n",
    "            idf = idf_values[token]\n",
    "            scores[doc][token] = tf * idf  \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_values=compute_tf_idf(idf,word_count_in_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "\n",
    "def get_top_words(tf_idf_scores, top_n=7):\n",
    "    top_scores = {}\n",
    "    for doc, scores in tf_idf_scores.items():\n",
    "        sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_scores[doc] = dict(sorted_scores[:top_n])\n",
    "    return top_scores\n",
    "\n",
    "def reshape_arabic_text(text):\n",
    "    return get_display(arabic_reshaper.reshape(text))\n",
    "\n",
    "def reshape_arabic_words(words):\n",
    "    reshaped_words = [get_display(arabic_reshaper.reshape(word)) for word in words]\n",
    "    return reshaped_words\n",
    "\n",
    "def create_combined_heatmap(top_scores, max_docs=7):\n",
    "    reshaped_docs = []\n",
    "    reshaped_words = set()\n",
    "\n",
    "    selected_docs = list(top_scores.keys())[:max_docs]\n",
    "    data_dict = {}\n",
    "    for doc in selected_docs:\n",
    "        reshaped_doc = reshape_arabic_text(doc)\n",
    "        reshaped_docs.append(reshaped_doc)\n",
    "\n",
    "        for word, score in top_scores[doc].items():\n",
    "            reshaped_word = get_display(arabic_reshaper.reshape(word))\n",
    "            reshaped_words.add(reshaped_word)\n",
    "\n",
    "            if reshaped_word not in data_dict:\n",
    "                data_dict[reshaped_word] = {}\n",
    "            data_dict[reshaped_word][reshaped_doc] = score\n",
    "    data_df = pd.DataFrame.from_dict(data_dict, orient=\"index\", columns=reshaped_docs).fillna(0)\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(max_docs * 2, len(data_df) * 0.5))\n",
    "    sns.heatmap(\n",
    "        data_df,\n",
    "        cmap=\"YlGnBu\",\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={\"label\": \"TF-IDF Score\"}\n",
    "    )\n",
    "    plt.title(\"Combined TF-IDF Heatmap\")\n",
    "    plt.xlabel(\"Documents\")\n",
    "    plt.ylabel(\"Words\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores=get_top_words(tf_idf_values)\n",
    "create_combined_heatmap(top_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 and 150 words that are frequent across documents (not cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_frequent_idf=find_top_100(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_150_frequent_idf=find_top_150(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 100 and 150 words that are frequent across documents (cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_freq_idf_cleaned= find_top_100(idf_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_150_freq_idf_cleaned= find_top_150(idf_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud For (all docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import arabic_reshaper\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from bidi.algorithm import get_display\n",
    "\n",
    "def generate_wordcloud_from_dict(word_counts):\n",
    "    font_directory = os.path.join(os.getcwd(), \"font\")\n",
    "    for file in os.listdir(font_directory):\n",
    "        if file.endswith(\".ttf\"): \n",
    "            font_file = os.path.join(font_directory, file)\n",
    "            break\n",
    "    if not font_file:\n",
    "        raise FileNotFoundError(\"No .ttf font file found in the specified directory!\")\n",
    "    \n",
    "    reshaped_word_counts = {\n",
    "        get_display(arabic_reshaper.reshape(word)): count for word, count in word_counts.items()\n",
    "    }\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        font_path=font_file,\n",
    "        width=5000,\n",
    "        height=4000,\n",
    "        random_state=1,\n",
    "        background_color='white',\n",
    "        colormap='Set2'\n",
    "    ).generate_from_frequencies(reshaped_word_counts)\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_from_dict(word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_from_dict(cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud for Rare Words in All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_cloud_rare(idf, word_count_in_all_doc):\n",
    "    reversed_idf = {key: idf[key] for key in reversed(idf)}\n",
    "    rare=dict()\n",
    "    counter=0\n",
    "    for entry in reversed_idf:\n",
    "        rare[entry]=word_count_in_all_doc[entry]\n",
    "        counter+=1\n",
    "        if(counter==100):\n",
    "            break\n",
    "    generate_wordcloud_from_dict(rare) \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_rare(idf,word_count_in_all_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_rare(idf_cleaned,cleaned_count_per_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_all_documents_after, doc_lengths_after= tokenize_all_documents(\"Preprocessed Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(doc_lengths_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bigram(tokenize_all_documents_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_in_doc_after= word_count_per_document(tokenize_all_documents_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_words_after= count_word_across_docs(word_count_in_doc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_doc_words_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choose_frequent_words_550(all_doc_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_after=calculate_idf(word_count_in_doc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud_from_dict(all_doc_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_word_cloud_rare(idf_after,all_doc_words_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_values_after= compute_tf_idf(idf_after,word_count_in_doc_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores_after= get_top_words(tf_idf_values_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_combined_heatmap(top_scores_after)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
